# -*- coding: utf-8 -*-
"""veribilimi.proje.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1840TAJ-Kin7BozcxzOuYwRkr24XZyDUl

# Veri Bilimine Giriş Dönem

## Problem Tanımı ve Proje Amacı

Müşteri Kişilik Analizi, bir şirketin ideal müşterilerinin detaylı bir analizidir. Bir işletmenin müşterilerini daha iyi anlamasına yardımcı olur ve farklı müşteri türlerinin özel ihtiyaçlarına, davranışlarına ve endişelerine göre ürünlerini değiştirmelerini kolaylaştırır.

Müşteri kişilik analizi, bir işletmenin ürününü farklı müşteri segmentlerinden hedef müşterilerine göre değiştirmesine yardımcı olur. Örneğin, bir şirket yeni bir ürünü şirketin veri tabanındaki her müşteriye pazarlamak için para harcamak yerine, hangi müşteri segmentinin ürünü satın alma olasılığının daha yüksek olduğunu analiz edebilir ve ardından ürünü yalnızca bu segmente pazarlayabilir.

Bu projede Farklı teknikler ile Müşteri kişilik analizi yaparak bir Clustring (Kümeleme) problemi çözeceğiz.

## Veri seti ve Parametreleri

Müşteri kişilik analizi problemi için kullanacağımız veri setini Kaggle üzerinden [bu linkten](https://www.kaggle.com/datasets/imakash3011/customer-personality-analysis/data
) elde ettik.


Müşteri segmentlerini özetlemek için kullanacağımız bu veri seti Dr. Omar Romero-Hernandez tarafından hazırlanmıştır.

Veri seti 4 farklı türde verileri içermektedir.
Veri seti parametrelerinin açıklamalarını buradan bulabilirsiniz.

### Parametreler

People

*   ID: Müşterinin benzersiz tanımlayıcısı
*   Year_Birth: Müşterinin doğum yılı
*   Education: Müşterinin eğitim seviyesi
*   Marital_Status: Müşterinin medeni durumu
*   Income: Müşterinin yıllık hane geliri
*   Kidhome: Müşterinin hanesindeki çocuk sayısı
*   Teenhome: Müşterinin hanesindeki genç sayısı
*   Dt_Customer: Müşterinin şirkete kayıt tarihi
*   Recency: Müşterinin son satın alımından bu yana geçen gün sayısı
*   Complain: Müşteri son 2 yıl içinde şikayette bulunduysa 1, aksi takdirde 0


Products

*   MntWines: Son 2 yılda şarap için harcanan miktar
*   MntFruits: Son 2 yılda meyveler için harcanan miktar
*   MntMeatProducts: Son 2 yılda et için harcanan miktar
*   MntFishProducts: Son 2 yılda balık için harcanan miktar
*   MntSweetProducts: Son 2 yılda şekerleme için harcanan miktar
*   MntGoldProds: Son 2 yılda altın için harcanan miktar


Promotion

*   NumDealsPurchases: İndirimle yapılan satın alma sayısı
*   AcceptedCmp1: Müşteri 1. kampanyada teklifi kabul ettiyse 1, aksi takdirde 0
*   AcceptedCmp2: Müşteri 2. kampanyada teklifi kabul ettiyse 1, aksi takdirde 0
*   AcceptedCmp3: Müşteri 3. kampanyada teklifi kabul ettiyse 1, aksi takdirde 0
*   AcceptedCmp4: Müşteri 4. kampanyada teklifi kabul ettiyse 1, aksi takdirde 0
*   AcceptedCmp5: Müşteri 5. kampanyada teklifi kabul ettiyse 1, aksi takdirde 0
*   Response: Müşteri son kampanyada teklifi kabul etmişse 1, aksi takdirde 0


Place

*   NumWebPurchases: Şirketin web sitesi üzerinden yapılan satın alma sayısı
*   NumCatalogPurchases: Katalog kullanılarak yapılan satın alma sayısı
*   NumStorePurchases: Doğrudan mağazalardan yapılan satın alma sayısı
*   NumWebVisitsMonth: Son bir ay içinde şirketin web sitesine yapılan ziyaret sayısı
"""

# Kütüphanelerin aktarımı
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px


palette = ["#a7e149","#e149a7", "#c393f4", "#d683f2", "#dd9e9e", "#fa91aa", "#fc9e9e", "#49e183", "#aa91aa", "#ff93f4"]

# Veri setinin içe aktarımı
df = pd.read_csv('marketing_campaign.csv', sep='\t')

df.head().T

"""## Veri İnceleme, Temizleme ve Özellik mühendisliği"""

df.info()

# Boş veri kontrolü
df.isna().sum()

# Boş verilerin atılması
df.dropna(inplace=True)

df['Dt_Customer'].head()

# Kayıt tarihinin veri dönüşümü ve incelenmesi
df['Dt_Customer'] = pd.to_datetime(df["Dt_Customer"],format='%d-%m-%Y')
dates = []
for i in df['Dt_Customer']:
    i = i.date()
    dates.append(i)

print(f"En yeni kayıtlı müşteri :- {max(dates)}")
print(f"En eski kayıtlı müşteri :- {min(dates)}")

# Kategorik verileri daha iyi anlayabilmek için metin düzenleme işlemine yazılan fonksiyon
def categorical_data(data):
    cat_columns = data.select_dtypes(['object']).columns

    for col in cat_columns:
        data[col] = data[col].str.strip() # Gereksiz boşlukları kaldırır
        data[col] = data[col].str.lower() # Tüm metinleri küçük harfe çevirir
        data[col] = data[col].str.replace('[^a-zA-Z0-9\s-]', '', regex=True) # metin düzenleme işlemini sağlar

    return data

df = categorical_data(df)

# Yaşa göre kuşakları grupladığımız fonksiyon
def assign_generation(year):
    if 1928 <= year <= 1945:
        return 'Silent Generation'
    elif 1946 <= year <= 1964:
        return 'Baby Boomers'
    elif 1965 <= year <= 1980:
        return 'Generation X'
    elif 1981 <= year <= 1996:
        return 'Millennials'
    elif 1997 <= year <= 2012:
        return 'Generation Z'
    else:
        return 'Generation Alpha'  # 2012 sonrasında doğanları kapsar

df['Generation'] = df['Year_Birth'].apply(assign_generation)

# Yıllık kazanılan miktara göre bütçe gruplandırması yaptığımız fonksiyon
def income_grouping(income):
    if income < 25000:
        return 'Below 25,000'
    elif income < 35000:
        return '25,001-35,000'
    elif income < 45000:
        return '35,001-45,000'
    elif income < 55000:
        return '45,001-55,000'
    elif income < 65000:
        return '55,001-65,000'
    elif income < 75000:
        return '65,001-75,000'
    else:
        return 'Above 75,000'

df['Income_Group'] = df['Income'].apply(income_grouping)

# Toplam kampanya yanıtlarını buluruz
df['TotalAcceptedCmps'] = df['AcceptedCmp1'] + df['AcceptedCmp2'] + df['AcceptedCmp3'] + df['AcceptedCmp4'] + df['AcceptedCmp5']

# Medeni durumlara verilen yanıtları kategorize ettiğimiz fonkisyon
def marital_grouping(status):
    if status in ['married', 'together']:
        return 'In Relationship'
    elif status in ['single', 'divorced', 'widow']:
        return 'Not In Relationship'
    else:
        return 'Undefined'

df['Marital_Group'] = df['Marital_Status'].apply(marital_grouping)

# Doğum yılına göre yaşları bularak yeni bir sütun olarak ekledik
df['Age'] = 2023 - df['Year_Birth']

# Son 30 gün içinde alış veriş yapmış olma durumunu yeni bir sütun olarak ekledik
df['in_last_month'] = df['Recency'].apply(lambda x: 'yes' if x <= 30 else 'no')

df.info()

df.head().T

# Unique sayılarımızı inceledik
df.nunique()

"""#### NOT: Yeni değişkenler yaratma sebebimiz kümeleme problemlerinde veri setini daha iyi anlamak, kümeleme algoritmalarının performansını artırmak ve anlamlı gruplamalar elde etmek amacıyla gerçekleştirdik"""

# Gereksiz sütunları kaldırdık
columns_to_drop = ['ID', 'Z_CostContact', 'Z_Revenue']
df.drop(columns_to_drop, axis=1, inplace=True)

df

"""## EDA

### Eğitim
Eğitim genellikle satın alma davranışlarını, tercihleri ve hatta marka bağlılıklarını etkileyebilir. Veri kümesinin eğitim niteliklerini nasıl ayırdığı aşağıda açıklanmıştır:

Mezuniyet - 1.116 üye:
Lisans derecelerini tamamlamış bireylerden oluşan en büyük grup. Bu grup geniş bir ilgi alanı ve satın alma davranışı yelpazesine sahip olabilir.

Doktora - 481 üye:
En yüksek akademik yeterliliğe sahip önemli bir kesim. Kaliteye, araştırmaya dayalı satın alımlara öncelik verebilir ve daha yüksek bir satın alma gücüne sahip olabilirler.

Yüksek Lisans - 365 üye:
Lisansüstü derecelere sahip olan bu kişilerin belirli zevklere ve potansiyel olarak daha yüksek harcanabilir gelire sahip profesyoneller olması muhtemeldir.

2n Döngüsü - 200 üye:
Bu segment, belirli bir eğitim sisteminin yeterliliğini temsil edebilir, belki de lisansüstü diplomaya veya bazı uzmanlık eğitimlerine eşdeğerdir.

Temel - 54 üye:
Temel eğitime sahip olanlar, satın alma tercihleri daha faydacı ve bütçe bilincine sahip olabilir.

Bu kategorilerin anlaşılması, işletmelere pazarlama stratejilerini her bir grubun kendine has özelliklerine ve tercihlerine göre uyarlama konusunda yardımcı olabilir.
"""

df['Education'].value_counts()

# Analizi kolaylaştırmak ve daha hızlı şekilde grafikler çizebilmek için oluşturduğumuz fonksiyon
def draw_plot(data, figsize=(8, 6), colors=None):
    """
6'dan az benzersiz gözlem içeren veriler için pasta grafik çizer.
Aksi takdirde bir çubuk grafik çizer.

    """

    if not isinstance(data, pd.Series):
        raise ValueError("Kategorik bir veri girmelisiniz.")

    counts = data.value_counts()

    fig, ax = plt.subplots(figsize=figsize)

    # 6'dan az benzersiz gözlem varsa, bir pasta grafik çiz
    if len(counts) <= 5:
        counts.plot(kind='pie', autopct='%1.1f%%', colors=colors, ax=ax)
        ax.set_title("Pie Chart of " + data.name)
        ax.set_ylabel("")
    else:
        # Aksi takdirde bar chart çiz
        sns.barplot(x=counts.index, y=counts.values, palette=colors, ax=ax)
        ax.set_title("Bar Chart of " + data.name)
        ax.set_ylabel("Count")

    plt.tight_layout()
    plt.show()

draw_plot(df['Education'])

plt.figure(figsize=(10, 8))
sns.countplot(data=df, x = 'Education', hue = 'Kidhome')
plt.show()

"""### Medeni Durum

İlişki durumu, satın alma davranışları ve yaşam tarzı seçimleri hakkında çok şey ortaya koyabilir. İşte medeni kategorilerin bir dökümü:

İlişki İçinde - 1.430 üye:
Evli olan veya bir partnerle birlikte yaşayanları kapsamaktadır. Bu bireyler genellikle ortak mali kaynaklara sahiptir ve ortak satın alma kararları alabilirler.

İlişkisi Olmayanlar - 779 üye:
Bekarları, boşanmışları ve dulları kapsar. Genellikle bireysel ihtiyaçları karşıladıkları için satın alma tercihleri farklılık gösterebilir.

Tanımsız - 7 üye:
Bu grup 'Yalnız', 'Saçma' veya 'YOLO' gibi geleneksel olmayan veya alışılmışın dışında ilişki durumlarından oluşur. Satın alma modelleri daha az tahmin edilebilir olabilir.
"""

df['Marital_Group'].value_counts()

draw_plot(df['Marital_Group'])

plt.figure(figsize=(10, 8))
sns.countplot(data=df, x = 'Marital_Group', hue = 'Kidhome')
plt.show()

"""### Kuşaklar
X Kuşağı (1965-1980) - 1061 üye:
Bebek patlamasından sonra doğdular, kişisel bilgisayarların ve MTV'nin ortaya çıkışıyla büyüdüler. Uyum sağlama yetenekleri nedeniyle değerlidirler.

Baby Boomers (1946-1964) - 749 üye:
İkinci Dünya Savaşı sonrası doğan bu grup, 20. yüzyılın önemli olaylarına tanıklık etmiştir. Sadakatleri ve yüz yüze etkileşimi tercih etmeleriyle bilinirler.

Y Kuşağı (1981-1996) - 380 üye:
Teknoloji meraklısı dijital yerliler, sahip olduklarından ziyade deneyimlere değer verirler ve daha liberal görüşlere sahiptirler.

Sessiz Kuşak (1928-1945) - 23 üye:
İki Dünya Savaşı arasında doğan bu kuşak, dayanıklılık ve gelenekçilik ile karakterize edilmektedir.

Alfa Kuşağı (2010'lar-2025'ler) - 3 üye:
Y kuşağının çocuklarıdır, tamamen 21. yüzyılda yetişmişlerdir ve teknolojiye son derece yatkın olmaları beklenir.
"""

df['Generation'].value_counts()

draw_plot(df['Generation'])

plt.figure(figsize=(10, 8))
sns.countplot(data=df, x = 'Generation', hue = 'Kidhome')
plt.show()

"""### Yaş
Müşterilerin yaşını temsil eden Yaş değişkeni, şirketin müşteri kitlesinin yaş demografisi hakkında önemli bilgiler sağlar.

Çarpıklık: Çarpıklık değeri 0.353661 ile yaş dağılımı hafif pozitif çarpıktır. Bu, veri setinin yaşlılara kıyasla biraz daha fazla sayıda genç müşteri içerdiğini, ancak önemli ölçüde olmadığını göstermektedir.
Yaş dağılımının yorumlanması, pazarlama kampanyalarının, ürün tekliflerinin ve hizmetlerin baskın yaş grubuna göre uyarlanmasına yardımcı olduğu için çok önemlidir
"""

df["Age"].agg(["min","mean","median","max","std","skew"]).to_frame().T

# Numeric dataların grafik çizimini kolaylaştırmak için oluşturduğumuz fonksiyon
def draw_numeric_plot(data):

    num_unique = data.nunique()

    if num_unique == 2:
        # Donut chart
        plt.figure(figsize=(8, 6))
        # Değer sayılarını ve etiketleri çıkarın
        counts = data.value_counts()
        labels = counts.index

        # Plot
        wedges, texts, autotexts = plt.pie(counts, labels=labels, wedgeprops=dict(width=0.4),
                                           autopct='%1.1f%%', startangle=140, pctdistance=0.85)

        # Merkez daire çizimi
        centre_circle = plt.Circle((0,0),0.70,fc='white')
        plt.gca().add_artist(centre_circle)

        # Metin görünürlüğünü ayarla
        for text, autotext in zip(texts, autotexts):
            text.set(size=12)
            autotext.set(size=12)

        plt.title(f"Donut Chart of {data.name}")
        plt.ylabel("")
        plt.legend(loc="best")

    elif 3 <= num_unique <= 50:
        # Bar chart
        plt.figure(figsize=(12, 6))
        data.value_counts().sort_index().plot(kind='bar', color='skyblue')
        plt.title(f"Bar Chart for {data.name}")
        plt.ylabel("Frequency")
        plt.xlabel(data.name)

    elif 51 <= num_unique < 90:
        # Basit histgram
        plt.figure(figsize=(12, 6))
        plt.hist(data, bins=30, edgecolor='black', color='skyblue')
        plt.title(f"Histogram of {data.name}")
        plt.ylabel("Frequency")
        plt.xlabel(data.name)

    elif num_unique >= 90:
        # Histogram ve boxplot
        fig, ax = plt.subplots(1, 2, figsize=(15, 6))

        ax[0].hist(data, bins=30, edgecolor='black', color='skyblue')
        ax[0].set_title(f"Histogram of {data.name}")
        ax[0].set_ylabel("Frequency")
        ax[0].set_xlabel(data.name)

        ax[1].boxplot(data, vert=False)
        ax[1].set_title(f"Boxplot of {data.name}")
        ax[1].set_yticklabels([data.name])

    plt.tight_layout()
    plt.show()

draw_numeric_plot(df['Age'])

plt.figure(figsize=(8, 6))
sns.kdeplot(data=df, x="Age", hue="Kidhome")
plt.show()

"""### Yakın Zamandaki Satın Alma Davranışı: Son Bir Ay İçerisindeki İşlemler
Satın Almadı (Hayır) - 1.498 müşteri:
Büyük bir kesim yakın zamanda işlem yapmadı ve etkileşimde potansiyel iyileştirme alanlarına işaret ediyor.

Alışveriş Yaptı (Evet) - 718 müşteri:
Oldukça büyük bir grup yakın zamanda etkileşimde bulunarak başarılı pazarlama veya cazip ürün tekliflerine işaret etti.
"""

df['in_last_month'].value_counts()

draw_numeric_plot(df['in_last_month'])

plt.figure(figsize=(10, 8))
sns.countplot(data=df, x = 'in_last_month', hue = 'Kidhome')
plt.show()

"""### Gelir Dağılımı
Veri["Income"] 6.763487'luk bir çarpıklığa sahiptir, bu da sağa çarpık bir dağılıma işaret eder. Bu, çoğu bireyin düşük ila ortalama aralıkta gelire sahip olmasına rağmen, önemli ölçüde daha yüksek gelire sahip daha küçük bir grup olduğunu göstermektedir. Bu yüksek gelirler ortalamayı yukarı çekerek medyanı bu veriler için potansiyel olarak daha temsili bir ölçü haline getirmektedir.
"""

df["Income"].agg(["min","mean","median","max","std","skew"]).to_frame().T

draw_numeric_plot(df['Income'])

plt.figure(figsize=(8, 6))
sns.kdeplot(data=df, x="Income", hue="Kidhome")
plt.show()

"""### Promosyonlara Yanıt Analizi
Yanıt, en son promosyon kampanyasının etkinliğine ilişkin içgörüler sunar:

Kabul Edilenler (333 Müşteri): Bu grup, en son promosyon teklifinin cazibesine kapılan ve bu teklifle etkileşime geçmeyi seçen müşterileri temsil etmektedir. Bu grubun varlığı, kampanyanın müşterilerimizin bir bölümüne hitap ettiğinin altını çizmektedir.

Reddedilenler (1883 Müşteri): Önemli ölçüde daha büyük bir grup, kampanyanın geniş bir erişime sahip olmasına rağmen yankısının seçici olduğunu gösteriyor. Bu durum pek çok şeye işaret edebilir: belki de teklif pek çok kişiye hitap etmiyordu, niş bir kesimi hedefliyordu ya da iletişim aracı etkili değildi.

Bu metrikleri derinlemesine incelemek, bu tür tepkilerin arkasındaki nedenleri aydınlatarak gelecekteki kampanyaları optimize etmek ve daha geniş bir kitleye hitap etmelerini sağlamak için değerli geri bildirimler sunabilir.
"""

df['Response'].value_counts()

draw_numeric_plot(df['Response'])

"""### Müşteri fırsat eğilimi
NumDealsPurchases, müşterilerin fırsat ve indirimlere olan eğilimlerine ışık tutuyor:

1 Fırsat (960 Müşteri): Belki de ilk keşifleri ya da seçici fırsat tercihlerini gösteren baskın bir segment.

2 Fırsat (493 Müşteri): Promosyon teklifleri ile devam eden etkileşimi göstermektedir.

3-5 Fırsat (575 Müşteri): Burada, belirli fırsatlarla seçici bir etkileşim olduğunu düşündüren bir düşüş görüyoruz.

6+ Fırsat (144 Müşteri): Bunlar, sayıları daha az olsa da her zaman pazarlık yapmaya hevesli, sık fırsat arayan müşterilerimizdir.

Fırsat Yok (44 Müşteri): Promosyonlardan etkilenmeyen küçük bir grup. Satın alma kararları başka faktörler tarafından yönlendiriliyor olabilir.

Bu dağılım, müşteri tabanımızdaki farklı promosyon iştahlarını vurgulamakta ve çok yönlü bir promosyon stratejisine ihtiyaç duyulduğunu göstermektedir.
"""

df['NumDealsPurchases'].value_counts()

draw_plot(df['NumDealsPurchases'])

"""#### Kampanya eğilimleri
TotalAcceptedCmps metriği, her bir müşterinin olumlu yanıt verdiği pazarlama kampanyalarının kümülatif sayısı hakkında bilgi sağlar.

0 kampanya: 1.757 müşterinin büyük çoğunluğu hiçbir kampanyayı kabul etmedi.

1 kampanya: 323 müşteri bir kampanyayı kabul etti.

2 kampanya: 81 müşteri iki kampanyaya yanıt verdi.

3 kampanya: 44 müşteri üç kampanyayla olumlu etkileşim kurdu.

4 kampanya: 11 müşteriden oluşan daha küçük bir grup dört kampanyayı kabul etmiştir.

Bu dağılım, müşterilerin pazarlama kampanyalarına ne kadar açık olduklarının bir resmini çizmektedir. Çoğunluğun kampanyalara yanıt vermemesi, pazarlama stratejisinin veya hedefleme yaklaşımının iyileştirilmesi için yer olabileceğini düşündürmektedir.
"""

df['TotalAcceptedCmps'].value_counts()

draw_plot(df['TotalAcceptedCmps'])

plt.figure(figsize=(8, 6))
sns.displot(data=df, x="TotalAcceptedCmps", hue="Response", col="Kidhome", kind="kde")
plt.show()

"""#### NOT: Yukarıdaki analiz daha da geliştirilebilir, spesifik olarak bir parametre üzerine incelemeler arttırılabilir. Biz incelemelerimize kümeleme işlemlerinden sonra kümeler bazında ilerleyeceğiz

# PCA analizi ve Kümeleme

PCA (Principal Component Analysis), temel bileşen analizi olarak da adlandırılan bir veri analizi yöntemidir. PCA, çok boyutlu veri setlerini daha az boyutlu bir forma dönüştürerek veri setinin içindeki temel varyansı yakalamayı amaçlar. Bu, veri setindeki karmaşıklığı azaltabilir ve önemli bilgileri korur.

PCA'nın temel amacı, veri setindeki değişkenlik maksimize edilerek yeni bir koordinat sistemine dönüştürmektir. Bu yeni koordinat sistemi, veri setindeki varyansın büyük bir kısmını koruyarak veri setini daha az boyutlu hale getirir. Bu, özellikle yüksek boyutlu veri setlerinde (çok sayıda değişken içeren) analiz ve görselleştirmenin daha kolay olmasını sağlar.
"""

df.head().T

"""PCA analizi için veri setini biraz daha küçülterek daha kategorik hale getiriyoruz"""

df['Marital_Group'].value_counts()

df["Children"]=df["Kidhome"]+df["Teenhome"]

df["Family_Size"] = df["Marital_Group"].replace({"Not In Relationship": 1, "In Relationship":2, "Undefined":1})+ df["Children"]


df["Frequency"]=df["NumDealsPurchases"] + df["NumWebPurchases"] + df["NumCatalogPurchases"] + df["NumStorePurchases"]

df["Monetary"] = df["MntWines"]+ df["MntFruits"]+ df["MntMeatProducts"]+ df["MntFishProducts"]+ df["MntSweetProducts"]+ df["MntGoldProds"]

del_cols = ["Marital_Status", 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response',
           "MntWines","MntFruits","MntMeatProducts","MntFishProducts","MntSweetProducts","MntGoldProds",
           "NumDealsPurchases","NumWebPurchases","NumCatalogPurchases","NumStorePurchases", 'NumWebVisitsMonth',
           "Year_Birth", "Kidhome","Teenhome"]
df =  df.drop(del_cols, axis=1)

df.head().T

df.info()

"""## Encoding

Encoding, kategorik verileri sayısal formata kodlamak için kullanılan bir ön işleme tekniğidir. Kategorik etiketleriniz veya sınıflarınız olduğunda kullanılır
* LabelEncoder, kategorik bir özellikteki her kategoriye benzersiz bir tamsayı atayarak kategorik verileri etkili bir şekilde sıralı verilere dönüştürür.
"""

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.cluster import AgglomerativeClustering
from yellowbrick.cluster import KElbowVisualizer

LE=LabelEncoder()
object_cols =["Education","Marital_Group","Generation", "Income_Group", "in_last_month"]
for i in object_cols:
    df[i]=df[[i]].apply(LE.fit_transform)
print('All features are now numerical')
df.head()

#Korelasyon matrisini hesapla
correlation_matrix = df.corr()

# Korelasyon matrisini ısı haritası ile görselleştir
plt.figure(figsize=(10, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)
plt.title('Korelasyon Matrisi Heatmap')
plt.show()

"""Children family size ile, recency, in_last month ile fazla korelasyona sahip Benzer etkileri olabileceğinden veri setinden birer tanesini ıkartıyoruz"""

df.drop(['Children',"in_last_month"],axis=1, inplace=True)
df

"""## Standardizasyon

Standartlaştırma, verileri 0 ortalama ve 1 standart sapmaya sahip olacak şekilde dönüştüren özel bir ölçeklendirme türüdür. * Standartlaştırma, verileri birçok makine öğrenimi algoritması ve istatistiksel test gibi Gauss dağılımı varsayan algoritmalar için uygun hale getirir. Ayrıca bazı modellerde farklı özelliklerin önemini vurgulamaya yardımcı olur.
"""

df_clustering=df[["Education","Age", "Recency","Family_Size","Monetary","Frequency","Income", "Generation","Marital_Group"]]

#Scaling
scaler = StandardScaler()
scaler.fit(df_clustering)
scaled_features = pd.DataFrame(scaler.transform(df_clustering),columns= df_clustering.columns )
print('Tüm featurelar scale edilmiştir')
scaled_features.head()

pca = PCA()
pca.fit(scaled_features)
cum_sum_eigenvalues = np.cumsum(pca.explained_variance_ratio_)

plt.figure(figsize=(10,5))
plt.bar(range(0, len(pca.explained_variance_ratio_)), pca.explained_variance_ratio_, label="Individual Explained Ratio")
plt.step(range(0, len(cum_sum_eigenvalues)), cum_sum_eigenvalues, label="Cumulative Explained Ratio")
plt.xlabel("N components")
plt.ylabel("Explained Variance Ratio")
plt.legend(loc="best")
plt.show()

"""Bizim veri setimiz için PCA analizinde ilk 5 bileşen genel küme dağılımı için (yakllaşık %90'ı) fikir vermektedir. Öznitelik sayımızı 5 olarak belirleyerek analize devam ettik."""

num_components = 5
selected_components = pca.components_[:num_components]

feature_names = scaled_features.columns
selected_feature_names = [feature_names[i] for i in range(num_components)]

print("En yüksek 5 öznitelik:", selected_feature_names)

cols = ["PCA1", "PCA2", "PCA3","PCA4","PCA5"]
pca = PCA(n_components=len(cols))
pca.fit(scaled_features)
PCA_df = pd.DataFrame(pca.transform(scaled_features), columns=(cols))
PCA_df.describe().T

"""K-Means algoritmasını kullanarak Elbow metodu ile en uygun veri kümesini belirlemeye çalıştık"""

model = KMeans()
visualizer = KElbowVisualizer(model, k=8)
visualizer.fit(PCA_df)
visualizer.show()

"""Silhouette score denetimsiz öğrenmede kümelerin kalitesini ölçmek için kullanılan bir metriktir. Bir kümedeki her bir veri noktasının, en yakın komşu kümeye kıyasla aynı kümedeki diğer veri noktalarına ne kadar benzer olduğunu ölçer."""

model = KMeans()
visualizer = KElbowVisualizer(model, k=8, metric='silhouette')
visualizer.fit(PCA_df)
visualizer.show()

from sklearn.metrics import silhouette_score
def visualize_silhouette_layer(data):
    clusters_range = range(2,10)
    results = []

    for i in clusters_range:
        km = KMeans(n_clusters=i, random_state=42)
        cluster_labels = km.fit_predict(data)
        silhouette_avg = silhouette_score(data, cluster_labels)
        results.append([i, silhouette_avg])

    result = pd.DataFrame(results, columns=["n_clusters", "silhouette_score"])
    pivot_km = pd.pivot_table(result, index="n_clusters", values="silhouette_score")

    plt.figure()
    sns.heatmap(pivot_km, annot=True, linewidths=1, fmt='.3f', cmap='RdYlGn')
    plt.tight_layout()
    plt.show()

visualize_silhouette_layer(PCA_df)

"""silhouette score nispeten yüksek ancak kümeleme geliştirilebilir. Ek olarak score değerlerinin birbirine yakın olması kümelerin homojen olarak da dağıldığını söyleyebilir

Elbow metodu ve silhuette score ile yaptığımız küme sayısı tahminleri birbirlerinden farklı çıkmıştır. Burada elbow metotu için çıkarttığımız grafiğin ve ilk yavaşlamaya başladığı noktayı, silhuette score ile anlamlı gelebilecek küme sayısını seçtik. Burada tahminleme yapacağımız küme sayısını 3 olarak belirledik.
"""

nb_clusters = visualizer.elbow_value_
AC = AgglomerativeClustering(n_clusters=3)#nb_clusters

# Model fitting and clustring predicted
AC_df = AC.fit_predict(PCA_df)
PCA_df["Clusters"] = AC_df

df["Clusters"]= AC_df

df["Clusters"] # Belirlenen kümeler

"""Küme tahminleme için pca analizi uygulanmış son veri setim 5 öznitelik taşıdığı için, en etkili 3 öznitelik için belirlenen kümeleri 3d uzayda incelemek istiyorum"""

import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot

from sklearn.decomposition import PCA

pca = PCA(n_components=3)
pca_result = pca.fit_transform(PCA_df[["PCA1", "PCA2", "PCA3", "PCA4", "PCA5"]])

PCA_df["PCA_result1"] = pca_result[:, 0]
PCA_df["PCA_result2"] = pca_result[:, 1]
PCA_df["PCA_result3"] = pca_result[:, 2]

x = PCA_df["PCA_result1"]
y = PCA_df["PCA_result2"]
z = PCA_df["PCA_result3"]
c = PCA_df["Clusters"]

trace1 = go.Scatter3d(x=x, y=y, z=z, mode='markers',
                    marker=dict(size=6, color=c, colorscale='Viridis', colorbar=dict(title='Clusters')))

data = [trace1]
layout = go.Layout(margin=dict(l=0, r=0, b=0, t=0))
fig = go.Figure(data=data, layout=layout)
iplot(fig)

fig = px.histogram(data_frame=df, x="Clusters", color="Clusters", color_discrete_sequence=palette,nbins=12)

fig.update_layout(title="Kümelerin dağılımı")
fig.update_xaxes(title_text="Clusters")
fig.update_yaxes(title_text="Count")

fig.show()

# PCA sütunlarına göre pairplot dağılımları
sns.pairplot(data=PCA_df, vars=['PCA1', 'PCA2', 'PCA3', "PCA4","PCA5"], hue='Clusters', palette=palette)
plt.suptitle("PCA sütunları için Pairplot", y=1.02)
plt.show()

"""Bazı parametrelerin kümeleme sonucunda uygun şekilde kümelendiğini bazı özelliklerin uygun şekilde kümelenmediğini görebiliyoruz"""

PCA_df

df

means = df.groupby(['Clusters']).agg({'Family_Size': 'mean',
                                                     'Income': 'mean',
                                                     'Age': 'mean',
                                                     'Recency': 'mean',
                                                     'Frequency': 'mean',
                                                     'Monetary': 'mean'})
counts = df['Clusters'].value_counts().reset_index()
counts.columns = ['Clusters', 'Count']

result = pd.merge(means, counts, on='Clusters')
result

"""Küme 0 :
* Bu kümenin toplam üye sayısı 970'dır
* Diğer kümelerle karşılaştırıldığında, düşük gelir ve düşük harcamaya  sahiptirler
* Aile büyüklükleri 2 ile 3 kişi arasındadır.
* Yaşları 50 ile 70 arasındadır.

Küme 1 :
* Bu kümenin toplam üyesi 756'tür
* Diğer kümelerle karşılaştırıldığında, ortalama Gelire ve ortalama harcamaya sahiptirler.
* Aile Büyüklükleri 2 ile 3 kişi arasındadır.
* Yaşları 50 ile 70 arasındadır.

Küme 2 :
* Bu kümenin toplam üye sayısı 490'dir.
* Diğer kümelere kıyasla yüksek gelire ve yüksek harcamaya sahiptirler.
* Aile büyüklükleri 1 ile 3 kişi arasındadır.
* Yaşları 45 ile 55 arasındadır.

PCA Analizine göre öznitelik azalttım ve k means algoritmasını kullanarak kümelerimi belirledim.

## RFM Analizi (Recency, Frequency, Monetary)

* Pazarlama ve müşteri ilişkileri yönetiminde kullanılan bir müşteri segmentasyonu ve analizi yöntemidir. Açılımı Recency, Frequency ve Monetary Value'dur ve müşterileri bu üç boyuta göre segmente etmeyi içerir.
* RFM analizi, işletmelerin müşteri davranışlarını anlamalarına ve değerli müşteri segmentlerini belirlemelerine yardımcı olur. Pazarlama stratejilerini kişiselleştirmek, yüksek değerli müşterileri hedeflemek ve müşteriyi elde tutmayı iyileştirmek için kullanılır.

* PCA Analizi ile belirlediğim kümelerim için RFM analizi kullanarak segmantasyon gerçekleştireceğiz.

RFM Analizi için, Recency Frequency ve Monetary verilerimi 5 eşit parçaya böldüm, daha düşük receny daha iyi bir durumu temsil ettiği için onu alt değerden itibaren büyüklük vermeye başladım.
"""

#RFM SCORES 2 boyutlu map üzerinden ilerledik, bu yüzden iki boyutlu bir skor hesapladık
def get_rfm_scores(dataframe):
    dataframe["R"] = pd.qcut(dataframe["Recency"], 5, labels=[5, 4, 3, 2, 1])
    dataframe["F"] = pd.qcut(dataframe["Frequency"].rank(method='first'), 5, labels=[1, 2, 3, 4, 5])
    dataframe["M"] = pd.qcut(dataframe["Monetary"], 5, labels=[1, 2, 3, 4, 5])
    dataframe["RFM_SCORE"] = dataframe["R"].astype(str) + dataframe["F"].astype(str) #+ dataframe["M"].astype(str)
    return dataframe

seg_map = {
    r'[1-2][1-2]': 'hibernating',
    r'[1-2][3-4]': 'at_Risk',
    r'[1-2]5': 'cant_loose',
    r'3[1-2]': 'about_to_sleep',
    r'33': 'need_attention',
    r'[3-4][4-5]': 'loyal_customers',
    r'41': 'promising',
    r'51': 'new_customers',
    r'[4-5][2-3]': 'potential_loyalists',
    r'5[4-5]': 'champions'
}

get_rfm_scores(df)
df.reset_index()

#RFM segment
df['segment'] = df['RFM_SCORE'].replace(seg_map, regex = True)
df.head()

x = df.segment.value_counts()


fig = px.treemap(x, path=[x.index], values=x, color=x.index,
                 color_discrete_map={x.index[i]: palette[i % len(palette)] for i in range(len(x.index))})


fig.update_layout(title_text='RFM Segmeantasyon Dağılımı', title_x=0.5,title_font=dict(size=20))
fig.update_traces(textinfo="label+value+percent root")
fig.show()

grouped = df.groupby(['Clusters', 'segment']).size().reset_index(name='Count')

pivot_table = grouped.pivot(index='segment', columns='Clusters', values='Count').fillna(0)

plt.figure(figsize=(10, 6))
sns.heatmap(pivot_table, annot=True, cmap="YlGnBu", fmt='g', cbar=True)
plt.xlabel('Clusters')
plt.ylabel('Segments')
plt.title('Segmentlerin kümeler içindeki dağılımı')
plt.show()

grouped = df.groupby(['Clusters', 'segment']).size().reset_index(name='Count')
pivot_table = grouped.pivot(index='segment', columns='Clusters', values='Count').fillna(0)

fig = px.bar(pivot_table, x=pivot_table.index, y=pivot_table.columns,
            title='Stacked Bar Plot', labels={'x': 'Clusters', 'y': 'Count'},
            color_discrete_sequence=palette)

fig.show()

grouped = df.groupby(['Clusters', 'segment']).size().reset_index(name='Count')
pivot_table = grouped.pivot(index='Clusters', columns='segment', values='Count').fillna(0)

fig = px.bar(pivot_table, x=pivot_table.index, y=pivot_table.columns,
             title='Kümelerdeki Segmantasyon Sayıları', labels={'index': 'Clusters', 'value': 'Count'},
             color_discrete_sequence=palette)


fig.show()

"""### Küme 0

* Bu kümenin toplam üye sayısı 970'dır
* Diğer kümelerle karşılaştırıldığında, düşük gelir ve düşük harcamaya  sahiptirler
* Aile büyüklükleri 2 ile 3 kişi arasındadır.
* Yaşları 50 ile 70 arasındadır.
* Yüksek sayıda "Kış uykusunda" (254) ve "Potansiyel sadık" (220) ile karakterize edilir. Ayrıca "uyumak üzere", "risk altında", "umut veren"  segmentlerine de sahiptir.
En değerli müşteriler olan "kış uykusunda" ve potansiyel olarak müdavim sayılabilecek "potansiyel sadıklar" tarafından domine edilir. Bu müşterileri korumak ve beslemek çok önemlidir. "kış uykusunda olan müşterilğeri uyuandırmak için yeni kampanyalar ve fırsatlar düzenlenebilir".


### Küme 1
* Bu kümenin toplam üyesi 756'tür
* Diğer kümelerle karşılaştırıldığında, ortalama Gelire ve ortalama harcamaya sahiptirler.
* Aile Büyüklükleri 2 ile 3 kişi arasındadır.
* Yaşları 50 ile 70 arasındadır.
* Bu küme, önemli sayıda "risk altındaki" müşteriler (149) ve "sadık" (143) ile çeşitli segmentlere sahiptir. Ayrıca "uyumak üzere", "risk altında", "şampiyonlar", "potansiyel sadık", "dikkat gerektiren", "yeni müşteriler" ve "gelecek vaat eden" segmentlerindeki müşterileri de içermektedir.
Hem potansiyel sadık müşteriler hem de dikkat gerektiren müşteriler dahil olmak üzere farklı müşteri segmentlerinin bir karışımına sahiptir. "Potansiyel_sadık müşterileri" elde tutmaya ve "risk altındaki" müşterilerle yeniden etkileşime geçmeye odaklanmak önemlidir.


### Küme 2
* Bu kümenin toplam üye sayısı 490'dir.
* Diğer kümelere kıyasla yüksek gelire ve yüksek harcamaya sahiptirler.
* Aile büyüklükleri 1 ile 3 kişi arasındadır.
* Yaşları 45 ile 55 arasındadır.
* "Risk Altında", "Kaybedemem", "Şampiyonlar", "Sadık Müşteriler", "Dikkat Gerektirenler" ve "Potansiyel Sadıklar" dahil olmak üzere daha az sayıda segmente sahiptir.
"Sadık müşteriler "e odaklanmaya ve bu segmentte büyüme fırsatlarını belirlemeye değer.
Sadakatlerini daha da güçlendirmek için "sadık müşteriler "in ihtiyaçlarını ve tercihlerini anlamaya çalışılabilir. Ayrıca, "kaybedemem", "şampiyonlar" ve "potansiyel sadık müşterileri" uzun vadeli sadık müşterilere dönüştürmenin yolları araştırılabilir.

Kazanılan gelire göre kümelerin dağılımları
"""

Personal = ["Education","Generation", "Income_Group", "TotalAcceptedCmps", "Marital_Group", "Age", "Family_Size"]

for i in Personal:
    plt.figure()
    sns.jointplot(x=df[i], y=df["Income"], hue =df["Clusters"], kind="kde")
    plt.show()

df["Monetary"] = df["Monetary"].astype(int)

"""Yapılan toplam harcamaya göre kümelerin dağılım bilgileri"""

Personal = ["Education","Generation", "Income_Group", "TotalAcceptedCmps", "Marital_Group", "Age", "Family_Size"]

for i in Personal:
    plt.figure()
    sns.jointplot(x=df[i], y=df["Monetary"], hue =df["Clusters"], kind="kde")
    plt.show()

